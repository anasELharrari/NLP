{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3393599-8ffc-4a55-8b3b-5d8268c517cb",
   "metadata": {},
   "source": [
    "# Word Embedding (WE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b432060-b4ad-426b-b9fc-91c1f1fd7bf4",
   "metadata": {},
   "source": [
    "## Gensim testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c17df2b-4ce5-4424-9a09-c0e8aaab0c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.test.gensim_fixt import setup_module\n",
    "setup_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fdf4c16-669f-4e3a-b57d-c313925687e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5f1584a-2177-4551-b02b-17309d6ec9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import brown\n",
    "\n",
    "train_set = brown.sents()[:10000]\n",
    "model = gensim.models.Word2Vec(train_set)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a66eb6e4-831e-432c-81f5-3e7227992756",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('brown.embedding')\n",
    "new_model = gensim.models.Word2Vec.load('brown.embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18089c4f-3692-49c7-b354-226dc8902178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_model.wv['university'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d249c20-7899-4d44-a8f6-470fa55cc8cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.wv.similarity('university','school') > 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "36ee8f05-67de-4516-8054-3abe0e2594a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.data import find\n",
    "\n",
    "word2vec_sample = str(find('models/word2vec_sample/pruned.word2vec.txt'))\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format(word2vec_sample, binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a19c3fcf-0fec-49cd-a848-bb16b6f1f282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model)\n",
    "len(model['university'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85f40593-4221-4167-aeaf-1cda65442db2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('universities', 0.7003918290138245),\n",
       " ('faculty', 0.6780906915664673),\n",
       " ('undergraduate', 0.6587096452713013)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['university'], topn = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7133b621-525b-4099-a470-e2e7d14e6216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cereal'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " model.doesnt_match('breakfast cereal dinner lunch'.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2bce6d-b833-4e55-8b78-fb68010d4948",
   "metadata": {},
   "source": [
    "## Spacy testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eadc583a-9077-40ee-ad8a-0088c7c02a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "     -------------------------------------- 12.8/12.8 MB 321.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.3.0) (3.3.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.6)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.10.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.11.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.12)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.28.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.15)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.9)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (63.4.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82137c5a-cf8f-4708-a681-50a87a0d9203",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.en.examples import sentences\n",
    "\n",
    "model2 =spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0e0c2cf-2df1-414f-9fb2-bc07af390851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Apple is looking at buying U.K. startup for $1 billion"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=model2(sentences[0])\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2c0a27a-2e30-436a-a760-0103cc6adfeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN nsubj Xxxxx\n",
      "is AUX aux xx\n",
      "looking VERB ROOT xxxx\n",
      "at ADP prep xx\n",
      "buying VERB pcomp xxxx\n",
      "U.K. PROPN compound X.X.\n",
      "startup NOUN dobj xxxx\n",
      "for ADP prep xxx\n",
      "$ SYM quantmod $\n",
      "1 NUM compound d\n",
      "billion NUM pobj xxxx\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_,token.shape_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3718f6b5-ef07-4bd5-b6d9-ddc912a83283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple 1.0\n",
      "Apple is -0.03927920013666153\n",
      "Apple looking -0.1143060103058815\n",
      "Apple at -0.16858862340450287\n",
      "Apple buying -0.044184841215610504\n",
      "Apple U.K. 0.2328547090291977\n",
      "Apple startup 0.12263332307338715\n",
      "Apple for -0.1312788426876068\n",
      "Apple $ -0.006184321828186512\n",
      "Apple 1 0.12815910577774048\n",
      "Apple billion -0.03881775960326195\n",
      "is Apple -0.03927920013666153\n",
      "is is 1.0\n",
      "is looking 0.1738957315683365\n",
      "is at 0.001597534865140915\n",
      "is buying 0.004276538733392954\n",
      "is U.K. -0.07689043879508972\n",
      "is startup -0.07900673896074295\n",
      "is for -0.09975917637348175\n",
      "is $ -0.040489550679922104\n",
      "is 1 0.03606772795319557\n",
      "is billion -0.1092299073934555\n",
      "looking Apple -0.1143060103058815\n",
      "looking is 0.1738957315683365\n",
      "looking looking 1.0\n",
      "looking at 0.05799964442849159\n",
      "looking buying 0.5491272807121277\n",
      "looking U.K. -0.03640978783369064\n",
      "looking startup 0.05116286128759384\n",
      "looking for -0.08290843665599823\n",
      "looking $ -0.2658574879169464\n",
      "looking 1 -0.13110095262527466\n",
      "looking billion 0.12498926371335983\n",
      "at Apple -0.16858862340450287\n",
      "at is 0.001597534865140915\n",
      "at looking 0.05799964442849159\n",
      "at at 1.0\n",
      "at buying 0.15815752744674683\n",
      "at U.K. 0.06366369128227234\n",
      "at startup 0.0023005218245089054\n",
      "at for 0.5995156168937683\n",
      "at $ -0.011958418413996696\n",
      "at 1 -0.14692218601703644\n",
      "at billion -0.1028519943356514\n",
      "buying Apple -0.044184841215610504\n",
      "buying is 0.004276538733392954\n",
      "buying looking 0.5491272807121277\n",
      "buying at 0.15815752744674683\n",
      "buying buying 1.0\n",
      "buying U.K. 0.07472673058509827\n",
      "buying startup 0.13119320571422577\n",
      "buying for 0.007753906771540642\n",
      "buying $ -0.0737537369132042\n",
      "buying 1 0.01394911389797926\n",
      "buying billion 0.05737799033522606\n",
      "U.K. Apple 0.2328547090291977\n",
      "U.K. is -0.07689043879508972\n",
      "U.K. looking -0.03640978783369064\n",
      "U.K. at 0.06366369128227234\n",
      "U.K. buying 0.07472673058509827\n",
      "U.K. U.K. 1.0\n",
      "U.K. startup 0.27055054903030396\n",
      "U.K. for 0.10980632901191711\n",
      "U.K. $ 0.05854977294802666\n",
      "U.K. 1 0.21953125298023224\n",
      "U.K. billion 0.08708538115024567\n",
      "startup Apple 0.12263332307338715\n",
      "startup is -0.07900673896074295\n",
      "startup looking 0.05116286128759384\n",
      "startup at 0.0023005218245089054\n",
      "startup buying 0.13119320571422577\n",
      "startup U.K. 0.27055054903030396\n",
      "startup startup 1.0\n",
      "startup for 0.11235854774713516\n",
      "startup $ 0.03989209607243538\n",
      "startup 1 0.037852369248867035\n",
      "startup billion 0.16702313721179962\n",
      "for Apple -0.1312788426876068\n",
      "for is -0.09975917637348175\n",
      "for looking -0.08290843665599823\n",
      "for at 0.5995156168937683\n",
      "for buying 0.007753906771540642\n",
      "for U.K. 0.10980632901191711\n",
      "for startup 0.11235854774713516\n",
      "for for 1.0\n",
      "for $ 0.21010255813598633\n",
      "for 1 -0.1653260439634323\n",
      "for billion -0.10392999649047852\n",
      "$ Apple -0.006184321828186512\n",
      "$ is -0.040489550679922104\n",
      "$ looking -0.2658574879169464\n",
      "$ at -0.011958418413996696\n",
      "$ buying -0.0737537369132042\n",
      "$ U.K. 0.05854977294802666\n",
      "$ startup 0.03989209607243538\n",
      "$ for 0.21010255813598633\n",
      "$ $ 1.0\n",
      "$ 1 0.3923080563545227\n",
      "$ billion 0.06346908956766129\n",
      "1 Apple 0.12815910577774048\n",
      "1 is 0.03606772795319557\n",
      "1 looking -0.13110095262527466\n",
      "1 at -0.14692218601703644\n",
      "1 buying 0.01394911389797926\n",
      "1 U.K. 0.21953125298023224\n",
      "1 startup 0.037852369248867035\n",
      "1 for -0.1653260439634323\n",
      "1 $ 0.3923080563545227\n",
      "1 1 1.0\n",
      "1 billion 0.46756815910339355\n",
      "billion Apple -0.03881775960326195\n",
      "billion is -0.1092299073934555\n",
      "billion looking 0.12498926371335983\n",
      "billion at -0.1028519943356514\n",
      "billion buying 0.05737799033522606\n",
      "billion U.K. 0.08708538115024567\n",
      "billion startup 0.16702313721179962\n",
      "billion for -0.10392999649047852\n",
      "billion $ 0.06346908956766129\n",
      "billion 1 0.46756815910339355\n",
      "billion billion 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_10888\\3090497972.py:3: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
      "  print(x.text , y.text , x.similarity(y))\n"
     ]
    }
   ],
   "source": [
    "for x in doc:\n",
    "    for y in doc:\n",
    "        print(x.text , y.text , x.similarity(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cad33d76-bbed-4330-ab0e-e86dc25e6d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apple True 7.164267 True\n",
      "looking True 8.166646 True\n",
      "billion True 7.531427 True\n",
      "hghdrj True 5.694851 True\n"
     ]
    }
   ],
   "source": [
    "new = model2(u'apple looking billion hghdrj')\n",
    "\n",
    "for i in new :\n",
    "    print(i.text , i.has_vector , i.vector_norm , i.is_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f124d9ca-6c7c-4c9c-83c7-e34a008d3efc",
   "metadata": {},
   "source": [
    "# Part2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dec0051b-9eba-42b8-bc72-4ff8098e99aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('Data.csv',encoding='latin1',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c6a61b-0eb1-4625-b461-bce9b346b1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "head(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd42f621-4d27-4e78-9344-2904d61e48a9",
   "metadata": {},
   "source": [
    "#### Data Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb15374-9105-4aba-b60f-bc07db4f4fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(data)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "term_frequencies = {dictionary[idx]: freq for idx, freq in dictionary.dfs.items()}\n",
    "sorted_terms = sorted(term_frequencies.items(), key=lambda x: x[1], reverse=True)\n",
    "for term, freq in sorted_terms[:10]:\n",
    "    print(term, freq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b291de6-67b6-4cd5-a6ba-2c7cc5af2429",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud(width=600, height=200).generate(corpus)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.imshow(wordcloud, interpolation='linear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758e0d1c-6d3a-46de-9635-94ce327ed108",
   "metadata": {},
   "source": [
    "## Word Embedding : Word2Vec "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d6499-cdc8-4ebc-a4f5-72ffc492d666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "model = Word2Vec(tokenized_text, vector_size=150, window=5, min_count=1, workers=4)\n",
    "\n",
    "vectors = []\n",
    "for tokens in data:\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if vectors:\n",
    "        vector = sum(vectors) / len(vectors)  \n",
    "        vectors.append(vector)\n",
    "    else:\n",
    "        vectors.append([])\n",
    "\n",
    "\n",
    "padded_vectors = pad_sequences(vectors, padding='post', dtype='float32')\n",
    "\n",
    "\n",
    "target = data['author']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(padded_vectors, target, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "clf = SVC()\n",
    "\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "result = clf.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
