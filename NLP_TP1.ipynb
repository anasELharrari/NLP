{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "843bc119-debc-4693-b24f-55c1ed6d1b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79ae22a-1af0-4c04-b472-6698c1833140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def levenshteinDistance(word1,word2) :\n",
    "    a=len(word1)\n",
    "    b=len(word2)\n",
    "    word1=[k for k in word1]\n",
    "    word2=[k for k in word2]\n",
    "    solution=np.zeros((a,b))\n",
    "    solution[0]=[n for n in range(a)]\n",
    "    solution[:,0]=[n for n in range(b)]\n",
    "    if word1[1]!=word2[1]:\n",
    "        solution[1,1]=2\n",
    "    for i in range(1,b):\n",
    "        for j in range(1,a):\n",
    "            if word1[j]!=word2[i]:\n",
    "                solution[j,i]=min(solution[j-1,i],solution[j,i-1])+1\n",
    "            else:\n",
    "                solution[j,i]=solution[j-1,i-1]\n",
    "                \n",
    "    return solution\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2df5533b-2d1c-4b78-83f8-a0fb61187113",
   "metadata": {},
   "outputs": [],
   "source": [
    "sol1=levenshteinDistance(\"correct\",\"correct\")\n",
    "sol2=levenshteinDistance(\"test\",\"tset\")\n",
    "sol3=levenshteinDistance(\"PIANO\",\"PIANA\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5b915c9-23f5-4b25-85a4-867829a7435d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('treebank')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbb3babc-2b42-436f-a069-048f1058dd1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK is a leading platform for building Python programs to work with human language data.',\n",
       " 'It provides easy-to-use interfaces to over 50 corpora...']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test1=\"NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora...\"\n",
    "sentence_tokenizing=nltk.sent_tokenize(test1)\n",
    "sentence_tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3f7230b-4214-4a49-8927-12a0ff9f9560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'intellegence',\n",
       " 'is',\n",
       " 'concedered',\n",
       " 'nowdays',\n",
       " 'as',\n",
       " 'an',\n",
       " 'interesting',\n",
       " 'feild',\n",
       " 'that',\n",
       " 'may',\n",
       " 'leads',\n",
       " 'to',\n",
       " 'some',\n",
       " 'great',\n",
       " 'inventions',\n",
       " ',',\n",
       " 'EL',\n",
       " 'HARRARI',\n",
       " 'ANAS',\n",
       " 'AI',\n",
       " 'Major',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence=\"\"\"Artificial intellegence is concedered nowdays as an interesting feild that may leads to some great inventions , EL HARRARI ANAS AI Major.\"\"\"\n",
    "tokens = nltk.word_tokenize(test_sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99a758f2-068c-4402-97f2-4f627d103f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Artificial', 'JJ'),\n",
       " ('intellegence', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('concedered', 'VBN'),\n",
       " ('nowdays', 'RB'),\n",
       " ('as', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('interesting', 'JJ'),\n",
       " ('feild', 'NN'),\n",
       " ('that', 'WDT'),\n",
       " ('may', 'MD'),\n",
       " ('leads', 'VB'),\n",
       " ('to', 'TO'),\n",
       " ('some', 'DT'),\n",
       " ('great', 'JJ'),\n",
       " ('inventions', 'NNS'),\n",
       " (',', ','),\n",
       " ('EL', 'NNP'),\n",
       " ('HARRARI', 'NNP'),\n",
       " ('ANAS', 'NNP'),\n",
       " ('AI', 'NNP'),\n",
       " ('Major', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e00ea564-eb03-42c5-a0df-0e73507862e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,1432.0,168.0\" width=\"1432px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"6.70391%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Artificial</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"3.35196%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.82123%\" x=\"6.70391%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">intellegence</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.6145%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.7933%\" x=\"14.5251%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">is</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.9218%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.70391%\" x=\"17.3184%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">concedered</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20.6704%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.02793%\" x=\"24.0223%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">nowdays</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">RB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.5363%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.23464%\" x=\"29.0503%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">as</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"30.1676%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.23464%\" x=\"31.2849%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">an</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"32.4022%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.26257%\" x=\"33.5196%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">interesting</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.1508%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.91061%\" x=\"40.7821%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">feild</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"42.7374%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.35196%\" x=\"44.6927%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">that</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">WDT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"46.3687%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.7933%\" x=\"48.0447%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">may</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">MD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"49.4413%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.91061%\" x=\"50.838%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">leads</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"52.7933%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.23464%\" x=\"54.7486%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">to</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">TO</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"55.8659%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.35196%\" x=\"56.9832%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">some</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"58.6592%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.91061%\" x=\"60.3352%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">great</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"62.2905%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"6.70391%\" x=\"64.2458%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">inventions</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"67.5978%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"1.67598%\" x=\"70.9497%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"71.7877%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.82123%\" x=\"72.6257%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ORGANIZATION</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">EL</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.5363%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"7.82123%\" x=\"80.4469%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ORGANIZATION</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">HARRARI</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"84.3575%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.35196%\" x=\"88.2682%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">ANAS</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"89.9441%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.7933%\" x=\"91.6201%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">AI</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"93.0168%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.91061%\" x=\"94.4134%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Major</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"96.3687%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"1.67598%\" x=\"98.324%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"99.162%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [('Artificial', 'JJ'), ('intellegence', 'NN'), ('is', 'VBZ'), ('concedered', 'VBN'), ('nowdays', 'RB'), ('as', 'IN'), ('an', 'DT'), ('interesting', 'JJ'), ('feild', 'NN'), ('that', 'WDT'), ('may', 'MD'), ('leads', 'VB'), ('to', 'TO'), ('some', 'DT'), ('great', 'JJ'), ('inventions', 'NNS'), (',', ','), Tree('ORGANIZATION', [('EL', 'NNP')]), Tree('ORGANIZATION', [('HARRARI', 'NNP')]), ('ANAS', 'NNP'), ('AI', 'NNP'), ('Major', 'NNP'), ('.', '.')])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import svgling\n",
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cc39acf-cade-4a64-b7c3-ad89b7dc603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import treebank\n",
    "t = treebank.parsed_sents('wsj_0001.mrg')[0]\n",
    "t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eab92012-c75b-4ea3-8e01-03644078c9d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"English\"))\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9fe81cee-7c4d-4bc2-b88b-a472011fc129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial',\n",
       " 'intellegence',\n",
       " 'concedered',\n",
       " 'nowdays',\n",
       " 'interesting',\n",
       " 'feild',\n",
       " 'may',\n",
       " 'leads',\n",
       " 'great',\n",
       " 'inventions',\n",
       " ',',\n",
       " 'EL',\n",
       " 'HARRARI',\n",
       " 'ANAS',\n",
       " 'AI',\n",
       " 'Major',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokens\n",
    "filtered_sentence=[]\n",
    "for w in tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        \n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b02117-7781-4432-8c7f-a663f3178f8f",
   "metadata": {},
   "source": [
    "stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d553c6c3-8daf-4841-bb78-504196172a39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "artifici\n",
      "intelleg\n",
      "is\n",
      "conced\n",
      "nowday\n",
      "as\n",
      "an\n",
      "interest\n",
      "feild\n",
      "that\n",
      "may\n",
      "lead\n",
      "to\n",
      "some\n",
      "great\n",
      "invent\n",
      ",\n",
      "el\n",
      "harrari\n",
      "ana\n",
      "ai\n",
      "major\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "for i in tokens:\n",
    "    print(ps.stem(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d295b6b-96c9-4220-b934-594050d23e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial intellegence is concedered nowdays as an interesting feild that may leads to some great inventions , EL HARRARI ANAS AI Major 56%.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"en\")\n",
    "test = nlp(\"Artificial intellegence is concedered nowdays as an interesting feild that may leads to some great inventions , EL HARRARI ANAS AI Major 56%.\")\n",
    "print(test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b3bf6be-9f92-4ca7-b9db-84a17ed47185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial\n"
     ]
    }
   ],
   "source": [
    "first_token = test[0]\n",
    "print(first_token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f235bd60-27f2-4561-a831-70c4276d8de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is concedered\n"
     ]
    }
   ],
   "source": [
    "slice_test = test[2:4]\n",
    "print(slice_test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "decf8779-a10d-4f06-902f-92479911a2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 56\n"
     ]
    }
   ],
   "source": [
    "for token in test:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = test[token.i + 1]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c91590b8-c827-455a-bd88-d5b08c85cab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.3.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.3.0/en_core_web_sm-3.3.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.4.0,>=3.3.0.dev0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.3.0) (3.3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.11.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.4.6)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.14 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.15)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.8)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.7.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.28.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (63.4.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.21.5)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.8.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.12)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.9)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.0.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from packaging>=20.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.0.9)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pathy>=0.3.5->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2023.5.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (3.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (0.4.5)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer<0.5.0,>=0.3.0->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.4.0,>=3.3.0.dev0->en-core-web-sm==3.3.0) (2.0.1)\n",
      "[+] Download and installation successful\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b483435f-0abc-471e-9038-c091ee48571a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial \n",
      "intellegence \n",
      "is \n",
      "concedered \n",
      "nowdays \n",
      "as \n",
      "an \n",
      "interesting \n",
      "feild \n",
      "that \n",
      "may \n",
      "leads \n",
      "to \n",
      "some \n",
      "great \n",
      "inventions \n",
      ", \n",
      "EL \n",
      "HARRARI \n",
      "ANAS \n",
      "AI \n",
      "Major \n",
      "56 \n",
      "% \n",
      ". \n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for token in test:\n",
    "    # Print the text and the predicted part-of-speech tag\n",
    "    print(token.text , token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "901a75e2-e122-4933-ac5a-53bb480c4790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial  \n",
      "intellegence  \n",
      "is  \n",
      "concedered  \n",
      "nowdays  \n",
      "as  \n",
      "an  \n",
      "interesting  \n",
      "feild  \n",
      "that  \n",
      "may  \n",
      "leads  \n",
      "to  \n",
      "some  \n",
      "great  \n",
      "inventions  \n",
      ",  \n",
      "EL  \n",
      "HARRARI  \n",
      "ANAS  \n",
      "AI  \n",
      "Major  \n",
      "56  \n",
      "%  \n",
      ".  \n"
     ]
    }
   ],
   "source": [
    "for token in test:\n",
    "    # Get the token text, part-of-speech tag and dependency label\n",
    "    token_text = token.text\n",
    "    token_pos = token.pos_\n",
    "    token_dep = token.dep_\n",
    "    print(token_text , token_pos , token_dep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9e2581d6-d2d3-42b2-9040-e88141567e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over the predicted entities\n",
    "for ent in test.ents:\n",
    "    # Print the entity text and its label\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c14432f-45ed-45ee-825f-42c442de25ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artificial\n",
      "intellegence\n",
      "is\n",
      "concedered\n",
      "nowdays\n",
      "as\n",
      "an\n",
      "interesting\n",
      "feild\n",
      "that\n",
      "may\n",
      "leads\n",
      "to\n",
      "some\n",
      "great\n",
      "inventions\n",
      ",\n",
      "EL\n",
      "HARRARI\n",
      "ANAS\n",
      "AI\n",
      "Major\n",
      "56\n",
      "%\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for token in test:\n",
    "    # Print the text tokens\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa22c480-7243-4578-8c7a-f58ffdb47e40",
   "metadata": {},
   "source": [
    "## Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b94cff7-6f24-4f29-8e5d-d4beae0e47c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En semaine, je me lève à 6h30, je prends une douche et un petit déjeuner et je pars au travail vers 7h15.\n",
      "\n",
      "Pour arriver à mon entreprise à 8 heures, il me faut environ 45 minutes en voiture, mais parfois j’arrive en retard à cause des embouteillages.\n",
      "\n",
      "Vers 10 heures, je vais boire un café en salle de pause.\n",
      "En semaine, je me lève à 6h30, je prends une douche et un petit déjeuner et je pars au travail vers 7h15.\n",
      "\n",
      "Pour arriver à mon entreprise à 8 heures, il me faut environ 45 minutes en voiture, mais parfois j’arrive en retard à cause des embouteillages.\n",
      "\n",
      "Vers 10 heures, je vais boire un café en salle de pause.\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "import spacy\n",
    "\n",
    "nltk_test=\"\"\"En semaine, je me lève à 6h30, je prends une douche et un petit déjeuner et je pars au travail vers 7h15.\n",
    "\n",
    "Pour arriver à mon entreprise à 8 heures, il me faut environ 45 minutes en voiture, mais parfois j’arrive en retard à cause des embouteillages.\n",
    "\n",
    "Vers 10 heures, je vais boire un café en salle de pause.\"\"\"\n",
    "print(nltk_test)\n",
    "\n",
    "nlp = spacy.blank(\"fr\")\n",
    "spacy_test = nlp(\"\"\"En semaine, je me lève à 6h30, je prends une douche et un petit déjeuner et je pars au travail vers 7h15.\n",
    "\n",
    "Pour arriver à mon entreprise à 8 heures, il me faut environ 45 minutes en voiture, mais parfois j’arrive en retard à cause des embouteillages.\n",
    "\n",
    "Vers 10 heures, je vais boire un café en salle de pause.\"\"\")\n",
    "\n",
    "print(spacy_test.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "794ab52b-f7a9-4632-b59e-eab30a5f2608",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['En', 'semaine', ',', 'je', 'me', 'lève', 'à', '6h30', ',', 'je', 'prends', 'une', 'douche', 'et', 'un', 'petit', 'déjeuner', 'et', 'je', 'pars', 'au', 'travail', 'vers', '7h15', '.', 'Pour', 'arriver', 'à', 'mon', 'entreprise', 'à', '8', 'heures', ',', 'il', 'me', 'faut', 'environ', '45', 'minutes', 'en', 'voiture', ',', 'mais', 'parfois', 'j', '’', 'arrive', 'en', 'retard', 'à', 'cause', 'des', 'embouteillages', '.', 'Vers', '10', 'heures', ',', 'je', 'vais', 'boire', 'un', 'café', 'en', 'salle', 'de', 'pause', '.']\n",
      "--- 0.003992795944213867 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "tokens = nltk.word_tokenize(nltk_test)\n",
    "print(tokens)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c146de0-904a-4c5c-ad76-16ade7a54086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En\n",
      "semaine\n",
      ",\n",
      "je\n",
      "me\n",
      "lève\n",
      "à\n",
      "6h30\n",
      ",\n",
      "je\n",
      "prends\n",
      "une\n",
      "douche\n",
      "et\n",
      "un\n",
      "petit\n",
      "déjeuner\n",
      "et\n",
      "je\n",
      "pars\n",
      "au\n",
      "travail\n",
      "vers\n",
      "7h15\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "Pour\n",
      "arriver\n",
      "à\n",
      "mon\n",
      "entreprise\n",
      "à\n",
      "8\n",
      "heures\n",
      ",\n",
      "il\n",
      "me\n",
      "faut\n",
      "environ\n",
      "45\n",
      "minutes\n",
      "en\n",
      "voiture\n",
      ",\n",
      "mais\n",
      "parfois\n",
      "j’\n",
      "arrive\n",
      "en\n",
      "retard\n",
      "à\n",
      "cause\n",
      "des\n",
      "embouteillages\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "Vers\n",
      "10\n",
      "heures\n",
      ",\n",
      "je\n",
      "vais\n",
      "boire\n",
      "un\n",
      "café\n",
      "en\n",
      "salle\n",
      "de\n",
      "pause\n",
      ".\n",
      "--- 0.03797626495361328 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for token in spacy_test:\n",
    "    print(token.text)\n",
    "    \n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "115e950f-482d-4e3e-84a9-5e7dfbfb8f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['En semaine, je me lève à 6h30, je prends une douche et un petit déjeuner et je pars au travail vers 7h15.', 'Pour arriver à mon entreprise à 8 heures, il me faut environ 45 minutes en voiture, mais parfois j’arrive en retard à cause des embouteillages.', 'Vers 10 heures, je vais boire un café en salle de pause.']\n",
      "--- 0.0019974708557128906 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "tokens = nltk.sent_tokenize(nltk_test)\n",
    "print(tokens)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4a752f46-fedb-4b47-9ea1-74d76d511d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "En semaine, je me lève à 6h30, je prends une douche et un petit déjeuner et je pars au travail vers 7h15.\n",
      "\n",
      "\n",
      "Pour arriver à mon entreprise à 8 heures, il me faut environ 45 minutes en voiture, mais parfois j’arrive en retard à cause des embouteillages.\n",
      "\n",
      "\n",
      "Vers 10 heures, je vais boire un café en salle de pause.\n",
      "--- 1.4016332626342773 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for sent in spacy_test.sents :\n",
    "    print(sent.text)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc73527-5120-4c1c-9ad9-59c969959d5c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
